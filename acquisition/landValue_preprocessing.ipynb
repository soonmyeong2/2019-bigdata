{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "import os\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as f\n",
    "import subprocess\n",
    "from pyspark.sql.functions import min, max, col\n",
    "from pyspark.sql.functions import regexp_extract, col\n",
    "from pyspark.sql.functions import concat, col, lit\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext()\n",
    "sqlcontext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/user/hadoop/data/landValue/landValue_14.csv', '/user/hadoop/data/landValue/landValue_15.csv', '/user/hadoop/data/landValue/landValue_16.csv', '/user/hadoop/data/landValue/landValue_17.csv', '/user/hadoop/data/landValue/landValue_18.csv']\n"
     ]
    }
   ],
   "source": [
    "dir_in = \"/user/hadoop/data/landValue\"\n",
    "args = \"hdfs dfs -ls \"+dir_in+\" | awk '{print $8}'\"\n",
    "proc = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n",
    "\n",
    "s_output, s_err = proc.communicate()\n",
    "all_dart_dirs = s_output.decode('utf-8').split()\n",
    "print(all_dart_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlcontext.read.format(\"com.databricks.spark.csv\")\\\n",
    "    .option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(all_dart_dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+----+\n",
      "|           CityCode|    Price|Year|\n",
      "+-------------------+---------+----+\n",
      "|1111018200101340012| 601600.0|2014|\n",
      "|1111018200101350000|2292000.0|2014|\n",
      "|1111018200101360001| 317100.0|2014|\n",
      "|1111018200101360002|1867000.0|2014|\n",
      "|1111018200101360003| 594000.0|2014|\n",
      "|1111018200101360004|2182000.0|2014|\n",
      "|1111018200101360005| 577500.0|2014|\n",
      "|1111018200101360006| 577500.0|2014|\n",
      "|1111018200101360007| 577500.0|2014|\n",
      "|1111018200101360008|1926000.0|2014|\n",
      "|1111018200101360009| 339300.0|2014|\n",
      "|1111018200101360010|1926000.0|2014|\n",
      "|1111018200101360011|1926000.0|2014|\n",
      "|1111018200101360012|1926000.0|2014|\n",
      "|1111018200101360013|1955000.0|2014|\n",
      "|1111018200101360014|1867000.0|2014|\n",
      "|1111018200101360015|1867000.0|2014|\n",
      "|1111018200101360016|1867000.0|2014|\n",
      "|1111018200101360017|1867000.0|2014|\n",
      "|1111018200101360018|1867000.0|2014|\n",
      "+-------------------+---------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4669061"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.show()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+\n",
      "|           CityCode|avg(Price)|\n",
      "+-------------------+----------+\n",
      "|1111018200101380019|  176140.0|\n",
      "|1111018300104760016| 2036800.0|\n",
      "|1111018400102510018|  448800.0|\n",
      "|1111018300104460011| 1857200.0|\n",
      "|1111018400101940005| 1338000.0|\n",
      "|1111018400101980001|  441540.0|\n",
      "|1111018200101270003| 1328200.0|\n",
      "|1111018100100160013| 4043000.0|\n",
      "|1111017900100420010| 4205400.0|\n",
      "|1111018200100810001|  353360.0|\n",
      "|1111018200102300039| 1862000.0|\n",
      "|1111018100100010117| 2566000.0|\n",
      "|1111017500102400016| 5128800.0|\n",
      "|1111016700101360002| 5497000.0|\n",
      "|1111018600101600003| 1357800.0|\n",
      "|1111017400106270074|  725340.0|\n",
      "|1111017300100011083| 1632200.0|\n",
      "|1111016400102390020| 4228000.0|\n",
      "|1111015900102530000| 3090000.0|\n",
      "|1111016700100310025| 6852800.0|\n",
      "+-------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "total_df = df.groupBy(\"CityCode\").agg(f.mean('Price'))\n",
    "total_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "967473"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'path hdfs://master:9000/user/hadoop/merge_LandValue already exists.;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/spark-2.2.2-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.2.2-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o36.csv.\n: org.apache.spark.sql.AnalysisException: path hdfs://master:9000/user/hadoop/merge_LandValue already exists.;\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:106)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:435)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:471)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:609)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:217)\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:597)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f0eec3f860d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtotal_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"merge_LandValue\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark-2.2.2-bin-hadoop2.7/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace)\u001b[0m\n\u001b[1;32m    764\u001b[0m                        \u001b[0mignoreLeadingWhiteSpace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignoreLeadingWhiteSpace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m                        ignoreTrailingWhiteSpace=ignoreTrailingWhiteSpace)\n\u001b[0;32m--> 766\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.2.2-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-2.2.2-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'path hdfs://master:9000/user/hadoop/merge_LandValue already exists.;'"
     ]
    }
   ],
   "source": [
    "total_df.write.csv(\"merge_LandValue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# landValue file merge\n",
    "sc.textFile(\"hdfs://master:9000/user/hadoop/merge_LandValue/part*\").coalesce(1).saveAsTextFile(\"hdfs://master:9000/user/hadoop/merge_LandValue_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2b_df = sqlcontext.read.option(\"header\",\"true\").csv(\"data/h2b.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+----+-----+----------+-----+--------+----+\n",
      "|     행정동코드|  시도명|시군구명| 읍면동명|     법정동코드|  동리명|    생성일자|말소일자|\n",
      "+----------+-----+----+-----+----------+-----+--------+----+\n",
      "|1100000000|서울특별시|null| null|1100000000|서울특별시|19880423|null|\n",
      "|1111000000|서울특별시| 종로구| null|1111000000|  종로구|19880423|null|\n",
      "|1111051500|서울특별시| 종로구|청운효자동|1111010100|  청운동|20081101|null|\n",
      "|1111051500|서울특별시| 종로구|청운효자동|1111010200|  신교동|20081101|null|\n",
      "|1111051500|서울특별시| 종로구|청운효자동|1111010300|  궁정동|20081101|null|\n",
      "|1111051500|서울특별시| 종로구|청운효자동|1111010400|  효자동|20081101|null|\n",
      "|1111051500|서울특별시| 종로구|청운효자동|1111010500|  창성동|20081101|null|\n",
      "|1111051500|서울특별시| 종로구|청운효자동|1111010800|  통인동|20081101|null|\n",
      "|1111051500|서울특별시| 종로구|청운효자동|1111010900|  누상동|20081101|null|\n",
      "|1111051500|서울특별시| 종로구|청운효자동|1111011000|  누하동|20081101|null|\n",
      "|1111051500|서울특별시| 종로구|청운효자동|1111011100|  옥인동|20081101|null|\n",
      "|1111051500|서울특별시| 종로구|청운효자동|1111011900|  세종로|20081101|null|\n",
      "|1111053000|서울특별시| 종로구|  사직동|1111010600|  통의동|19880423|null|\n",
      "|1111053000|서울특별시| 종로구|  사직동|1111010700|  적선동|19880423|null|\n",
      "|1111053000|서울특별시| 종로구|  사직동|1111011200|  체부동|19880423|null|\n",
      "|1111053000|서울특별시| 종로구|  사직동|1111011300|  필운동|19880423|null|\n",
      "|1111053000|서울특별시| 종로구|  사직동|1111011400|  내자동|19880423|null|\n",
      "|1111053000|서울특별시| 종로구|  사직동|1111011500|  사직동|19880423|null|\n",
      "|1111053000|서울특별시| 종로구|  사직동|1111011600|  도렴동|19981201|null|\n",
      "|1111053000|서울특별시| 종로구|  사직동|1111011700|  당주동|19981201|null|\n",
      "+----------+-----+----+-----+----------+-----+--------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21697"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2b_df.show()\n",
    "h2b_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+-----------------+\n",
      "|     법정동코드|collect_set(동리명)|collect_set(시군구명)|\n",
      "+----------+----------------+-----------------+\n",
      "|1144010400|           [도화동]|            [마포구]|\n",
      "|2711013800|         [계산동1가]|             [중구]|\n",
      "|2717010900|         [원대동3가]|             [서구]|\n",
      "|2820010500|           [서창동]|            [남동구]|\n",
      "|3014011400|           [태평동]|             [중구]|\n",
      "|3171025926|           [두산리]|            [울주군]|\n",
      "|3611034022|           [발산리]|               []|\n",
      "|4122025925|           [후사리]|            [평택시]|\n",
      "|4150036028|           [소고리]|            [이천시]|\n",
      "|4150037028|           [암산리]|            [이천시]|\n",
      "|4155033027|           [동촌리]|            [안성시]|\n",
      "|4159013700|            [송동]|            [화성시]|\n",
      "|4161033026|           [도웅리]|            [광주시]|\n",
      "|4180033022|           [백령리]|            [연천군]|\n",
      "|4180035026|           [동중리]|            [연천군]|\n",
      "|4215035026|           [남양리]|            [강릉시]|\n",
      "|4221000000|           [속초시]|            [속초시]|\n",
      "|4278036030|           [노동리]|            [철원군]|\n",
      "|4311112200|           [명암동]|        [청주시 상당구]|\n",
      "|4313011900|           [목행동]|            [충주시]|\n",
      "+----------+----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20560"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = h2b_df.groupBy(\"법정동코드\").agg(f.collect_set('동리명'), f.collect_set('시군구명'))\n",
    "test_df.show()\n",
    "test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21697"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_joined = test_df.join(h2b_df, test_df.법정동코드 == h2b_df.법정동코드, how='inner')\n",
    "_joined.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'total_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ef1df82f292e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bub'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregexp_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CityCode'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'(^[0-9]{10})'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'total_df' is not defined"
     ]
    }
   ],
   "source": [
    "t = total_df.withColumn('bub', regexp_extract('CityCode', '(^[0-9]{10})', 1))\n",
    "t.show()\n",
    "t.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "967473"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined = t.join(test_df, t.bub == test_df.법정동코드, how='inner')\n",
    "joined.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'udf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f95a94153ee3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjoin_udf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mudf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\",\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0ms_joined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"collect_set(동리명)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"collect_set(동리명)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"collect_set(시군구명)\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoin_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"collect_set(시군구명)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'udf' is not defined"
     ]
    }
   ],
   "source": [
    "join_udf = udf(lambda x: \",\".join(x))\n",
    "s_joined = joined.withColumn(\"collect_set(동리명)\", join_udf(col(\"collect_set(동리명)\")))\\\n",
    "        .withColumn(\"collect_set(시군구명)\", join_udf(col(\"collect_set(시군구명)\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = s_joined.withColumn('mountain', regexp_extract('CityCode', '^[0-9]{10}([0-9])', 1)).\\\n",
    "            withColumn('bon', regexp_extract('CityCode', '^[0-9]{11}([0-9]{4})', 1)).\\\n",
    "            withColumn('bu', regexp_extract('CityCode', '^[0-9]{15}([0-9]{4})', 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "in_df = t.withColumn(\"bon\", t[\"bon\"].cast(IntegerType()))\n",
    "in_df = in_df.withColumn(\"bu\", t[\"bu\"].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "_t = in_df.withColumn('moun', f.when(f.col('mountain') > '1', \"산\").otherwise(\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = _t.withColumn(\"Address\", concat( col(\"collect_set(시군구명)\"), lit(\" \"), col(\"collect_set(동리명)\"), lit(\" \") \\\n",
    "                            , col('moun') ,col('bon') )).select('Address', 'avg(Price)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|       Address|     avg(avg(Price))|\n",
      "+--------------+--------------------+\n",
      "|     중구 남창동 60|1.4366553846153846E7|\n",
      "|   중구 을지로6가 17|1.2686191048034934E7|\n",
      "|   용산구 한남동 620|   4557897.142857143|\n",
      "|  성동구 성수동1가 98|  3472657.1428571427|\n",
      "|   광진구 중곡동 129|           2479735.0|\n",
      "|   성북구 정릉동 132|  2463352.5714285714|\n",
      "|   성북구 정릉동 581|  1723336.6666666667|\n",
      "|   강북구 수유동 538|  1303412.9166666667|\n",
      "| 서대문구 북아현동 173|  3435000.0000000005|\n",
      "|   마포구 공덕동 464|           2331060.0|\n",
      "|   강서구 염창동 277|  3713199.6923076925|\n",
      "|   구로구 구로동 448|   1927122.528735632|\n",
      "|  영등포구 도림동 160|  2614118.1102362205|\n",
      "|   동작구 흑석동 276|           2115920.0|\n",
      "|  관악구 신림동 1556|  3620615.0943396227|\n",
      "|   강동구 상일동 404|           1018135.0|\n",
      "|서대문구 북아현동 1012|           2842900.0|\n",
      "|    종로구 봉익동 38|   5527752.380952381|\n",
      "|    중구 신당동 130|           6772925.0|\n",
      "| 성동구 성수동2가 333|   2917654.255319149|\n",
      "+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "106582"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = result_df.groupBy(\"Address\").agg(f.mean('avg(Price)'))\n",
    "test_df.show()\n",
    "test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "967473"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.write.csv(\"Address_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.textFile(\"hdfs://master:9000/user/hadoop/Address_/part*\").coalesce(1).saveAsTextFile(\"hdfs://master:9000/user/hadoop/Address__\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
